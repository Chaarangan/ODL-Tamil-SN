{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DistilBERT_base_multilingual_cased.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lzDhCukNDbLJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625058631733,"user_tz":-330,"elapsed":6009,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"825c29e8-566a-460f-afcb-e8e10a20ab43"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.5MB 7.7MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3MB 37.4MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 901kB 51.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vpi_lM3dklDn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625058639895,"user_tz":-330,"elapsed":8166,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"f38cd21d-d211-4312-b58a-92196447542f"},"source":["#mporting the necessary libraries\n","import torch\n","import time\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset,DataLoader\n","import pandas as pd\n","#!pip install transformers\n","from transformers import DistilBertModel, DistilBertTokenizer\n","from torch import cuda\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","import re\n","from nltk.corpus import words\n","from bs4 import BeautifulSoup\n","import nltk\n","nltk.download('words')\n","import nltk, string, re, spacy,unicodedata, random\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import ToktokTokenizer\n","import nltk, string, re, spacy,unicodedata, random\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUS6r42KRequ","executionInfo":{"status":"ok","timestamp":1625058658662,"user_tz":-330,"elapsed":18787,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"a101e7be-7080-4531-b7ce-17b51576bf1c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TqY1ECzYRUlI","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1625058908525,"user_tz":-330,"elapsed":661,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"dd3a6a95-c05e-4d28-e898-ba0893d8074a"},"source":["train = pd.read_csv('/content/drive/MyDrive/TASME/data/tamil_offensive_full_train.csv', names=['text','label','nan'])\n","train = train.drop(columns=['nan'])\n","train = train.dropna()\n","train.label = train.label.apply({'Not_offensive':0,'Offensive_Untargetede':1,'Offensive_Targeted_Insult_Group':2,'Offensive_Targeted_Insult_Individual':3,'not-Tamil':4, 'Offensive_Targeted_Insult_Other':5}.get)\n","train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>movie vara level la Erika poguthu</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I love Ajith Kumar Vivegam movie inki mjy bht ...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Padam nalla comedy padama irukum polaye..</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>karthick subburaj anne .... intha padam vetri ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>‡Æï‡Æµ‡ØÅ‡Æ£‡Øç‡Æü‡Æ∞‡Øç ‡Æ§‡Øá‡Æµ‡Æ∞‡Øç.‡Æö‡Ææ‡Æ∞‡Øç‡Æ™‡Ææ‡Æï ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø ‡Æ™‡ØÜ‡Æ± ‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç ü¶Å</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>35134</th>\n","      <td>Trending number #2 idhukku nammalam karanamnu ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>35135</th>\n","      <td>Movie script super, athuvum HIP HOP Tamizha mu...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>35136</th>\n","      <td>Just 3k likes for 300k likes</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>35137</th>\n","      <td>Aaloo le lo. Kanda le lo.</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>35138</th>\n","      <td>‡Æ®‡Ææ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æµ‡Æü‡Øç‡Æü‡ÆÆ‡Øç  ‡Æµ‡Æ©‡Øç‡Æ©‡Æø‡ÆØ‡Æ∞‡Øç ‡Æö‡Ææ‡Æ∞‡Øç‡Æ™‡Ææ‡Æï ‡Æ§‡Æø‡Æ∞‡Øå‡Æ™‡Æ§‡Æø ‡Æ™‡Æü...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>35139 rows √ó 2 columns</p>\n","</div>"],"text/plain":["                                                    text  label\n","0                      movie vara level la Erika poguthu      0\n","1      I love Ajith Kumar Vivegam movie inki mjy bht ...      4\n","2              Padam nalla comedy padama irukum polaye..      0\n","3      karthick subburaj anne .... intha padam vetri ...      0\n","4      ‡Æï‡Æµ‡ØÅ‡Æ£‡Øç‡Æü‡Æ∞‡Øç ‡Æ§‡Øá‡Æµ‡Æ∞‡Øç.‡Æö‡Ææ‡Æ∞‡Øç‡Æ™‡Ææ‡Æï ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø ‡Æ™‡ØÜ‡Æ± ‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç ü¶Å      0\n","...                                                  ...    ...\n","35134  Trending number #2 idhukku nammalam karanamnu ...      0\n","35135  Movie script super, athuvum HIP HOP Tamizha mu...      0\n","35136                       Just 3k likes for 300k likes      0\n","35137                          Aaloo le lo. Kanda le lo.      4\n","35138  ‡Æ®‡Ææ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æµ‡Æü‡Øç‡Æü‡ÆÆ‡Øç  ‡Æµ‡Æ©‡Øç‡Æ©‡Æø‡ÆØ‡Æ∞‡Øç ‡Æö‡Ææ‡Æ∞‡Øç‡Æ™‡Ææ‡Æï ‡Æ§‡Æø‡Æ∞‡Øå‡Æ™‡Æ§‡Æø ‡Æ™‡Æü...      0\n","\n","[35139 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Oc9yKKtbl572","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1625058909882,"user_tz":-330,"elapsed":5,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"e855cee9-832a-4bee-9cb9-0a70fc81e7cf"},"source":["test = pd.read_csv('/content/drive/MyDrive/TASME/data/tamil_offensive_full_test_with_labels.csv', names=['text', 'label'])\n","test.label = test.label.apply({'Not_offensive':0,'Offensive_Untargetede':1,'Offensive_Targeted_Insult_Group':2,'Offensive_Targeted_Insult_Individual':3,'not-Tamil':4, 'Offensive_Targeted_Insult_Other':5}.get)\n","test = test.dropna()\n","test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>14.12.2018 epo trailer pathutu irken ... Semay...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Paka thana poro movie la Enna irukunu</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>‚ÄúU kena tunggu lebih lama lagi untuk tahu saya...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Suriya anna vera level anna mass</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>suma kaththaatha da sound over a pooda kudaath...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4387</th>\n","      <td>‡ÆÆ‡Æ£‡Øç‡Æ£‡ØÅ ‡Æ™‡Øä‡Æ£‡Øç‡Æ£‡ØÅ ‡Æ∞‡ØÜ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øá ‡Æí‡Æ©‡Øç‡Æ©‡ØÅ ‡ÆÖ‡Æ§‡ØÅ‡Æ≤ ‡Æé‡Æµ‡Æ©‡Øç ‡Æï‡Øà‡ÆØ ‡Æµ‡Æö‡Øç‡Æö...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4388</th>\n","      <td>Babu mele ko ye song sunke kuch yesa feel hua ...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4389</th>\n","      <td>asuran= aadukalam+pudupettai+ wada chennai..ye...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4390</th>\n","      <td>Vijay's all movies look like same.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4391</th>\n","      <td>Eh Idhu 96</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4391 rows √ó 2 columns</p>\n","</div>"],"text/plain":["                                                   text  label\n","0     14.12.2018 epo trailer pathutu irken ... Semay...      0\n","1                 Paka thana poro movie la Enna irukunu      0\n","2     ‚ÄúU kena tunggu lebih lama lagi untuk tahu saya...      4\n","3                      Suriya anna vera level anna mass      0\n","4     suma kaththaatha da sound over a pooda kudaath...      1\n","...                                                 ...    ...\n","4387  ‡ÆÆ‡Æ£‡Øç‡Æ£‡ØÅ ‡Æ™‡Øä‡Æ£‡Øç‡Æ£‡ØÅ ‡Æ∞‡ØÜ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øá ‡Æí‡Æ©‡Øç‡Æ©‡ØÅ ‡ÆÖ‡Æ§‡ØÅ‡Æ≤ ‡Æé‡Æµ‡Æ©‡Øç ‡Æï‡Øà‡ÆØ ‡Æµ‡Æö‡Øç‡Æö...      2\n","4388  Babu mele ko ye song sunke kuch yesa feel hua ...      4\n","4389  asuran= aadukalam+pudupettai+ wada chennai..ye...      0\n","4390                 Vijay's all movies look like same.      0\n","4391                                         Eh Idhu 96      2\n","\n","[4391 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"Y8i8q3NBqhiW"},"source":["def deEmojify(string):\n","  emoji_pattern = re.compile(\"[\"\n","                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                              u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                              u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                              u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                              u\"\\U00002500-\\U00002BEF\"  # chinese char\n","                              u\"\\U00002702-\\U000027B0\"\n","                              u\"\\U00002702-\\U000027B0\"\n","                              u\"\\U000024C2-\\U0001F251\"\n","                              u\"\\U0001f926-\\U0001f937\"\n","                              u\"\\U00010000-\\U0010ffff\"\n","                              u\"\\u2640-\\u2642\"\n","                              u\"\\u2600-\\u2B55\"\n","                              u\"\\u200d\"\n","                              u\"\\u23cf\"\n","                              u\"\\u23e9\"\n","                              u\"\\u231a\"\n","                              u\"\\ufe0f\"  # dingbats\n","                              u\"\\u3030\"\n","                              \"]+\", flags=re.UNICODE)\n","  return emoji_pattern.sub(r'', string)\n","\n","def preprocess(text):\n","  text = deEmojify(text) #convert emojis to their defns in words, they might be useful\n","  text = re.sub(r'([\\.\\'\\\"\\/\\-\\_\\--])',' ', text) # remove punctuations , removes @USER / some abbreviatins\n","  to_remove_url = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n","      '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","  text = re.sub(to_remove_url,'',text)  # remove url patterns\n","  text = re.sub(\" \\d+\", \" \", text)\n","  text = text.replace(\",\",\" \")\n","  text = re.sub(r'(?:^| )\\w(?:$| )', ' ', text).strip()\n","  punctuation='!!\"$%&()*+-/:;<=>?[\\\\]^_{|}~.'\n","  text = ''.join(ch for ch in text if ch not in set(punctuation))\n","  # text = text.translate(str.maketrans('', '', string.punctuation))\n","  text = BeautifulSoup(text, 'html.parser').get_text()\n","    # Stopword Removing\n","  tokenizer = ToktokTokenizer()\n","  # convert sentence into token of words\n","  tokens = tokenizer.tokenize(text)\n","  tokens = [token.strip() for token in tokens]\n","  text = ' '.join(ch for ch in tokens)\n","  return text \n","\n","def clean(df):\n","  df['text'] = df['text'].apply(lambda x: preprocess(x))\n","\n","clean(train)\n","clean(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnBLWHNWx11a"},"source":["#Initializing the key variables which will be later used in the training\n","\n","MAX_LEN = 512\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 16\n","EPOCHS = 1\n","LEARNING_RATE = 0.01\n","distilbert_multilingual = 'distilbert-base-multilingual-cased'\n","distilbert_base_uncased = 'distilbert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(distilbert_multilingual,return_dict=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3fD_4oIyiDG"},"source":["class SentimentDataset(Dataset):\n","\n","  def __init__(self,dataframe,tokenizer,max_len):\n","    self.len = len(dataframe)\n","    self.data = dataframe\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len \n","  \n","\n","  def __getitem__(self,index):\n","    sentence = str(self.data.text[index])\n","    sentence = \" \".join(sentence.split())\n","    encoding = self.tokenizer.encode_plus(\n","        sentence,\n","        add_special_tokens = True,\n","        max_length = self.max_len,\n","        padding = 'max_length',\n","        return_token_type_ids = False,\n","        return_tensors = 'pt',\n","        truncation = True\n","    )\n","    #ids = encoding['input_ids']\n","    #mask = encoding['attention_mask']\n","    return {\n","        'ids' : encoding['input_ids'].flatten(),\n","        'mask': encoding['attention_mask'].flatten(),\n","        'targets': torch.tensor(self.data.label[index],dtype=torch.long)\n","    }\n","\n","  def __len__(self):\n","    return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xB_mASJm0tnH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625058921535,"user_tz":-330,"elapsed":6,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"94b641e1-5558-449d-99f8-67b6cdc38181"},"source":["#Creating the dataset and dataloader for training \n","train_size = 0.9\n","train_dataset = train.sample(frac=train_size,random_state=42)\n","test_dataset = train.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print('Total no of entities in the dataset: {}'.format(train.shape))\n","print('Train dataset:{}'.format(train_dataset.shape))\n","print('Test dataset: {}'.format(test_dataset.shape))\n","\n","training_set = SentimentDataset(train_dataset,tokenizer,MAX_LEN)\n","testing_set = SentimentDataset(test_dataset,tokenizer,MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total no of entities in the dataset: (35139, 2)\n","Train dataset:(31625, 2)\n","Test dataset: (3514, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0IAJRYDR1Wjb"},"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': False,\n","                'num_workers': 2\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': False,\n","                'num_workers': 2\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8EFGN972x96"},"source":["# Fine-Tuning DistilBERT by adding a dropout and a dense layer on top of it to get the final output\n","\n","class DistillBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(DistillBERTClass, self).__init__()\n","        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier = torch.nn.Linear(768, 6)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ycbjnIwDANGj"},"source":["###Trainning"]},{"cell_type":"code","metadata":{"id":"jdL0nZCT5Wfa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625058931426,"user_tz":-330,"elapsed":1854,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"f434d2ad-c25f-4873-fd31-182df983ca69"},"source":["model = DistillBERTClass()\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["DistillBERTClass(\n","  (auto): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lstm): LSTM(768, 256, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=128, bias=True)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.4, inplace=False)\n","  (out): Linear(in_features=128, out_features=6, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"mfqCZyyS5bp8"},"source":["#Defining the loss function and optimizer\n","loss_function = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.Adam(params= model.parameters(),lr = LEARNING_RATE)\n","#loss_function.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTjE1aDl57nx"},"source":["#Fine-Tuning DistilBERT\n","def calcuate_accuracy(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vs7HFEwZZV4Y"},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2AGaOSAd7zzG"},"source":["# Defining the training function on the 80% of the dataset for tuning the distilbert model\n","\n","def train(epoch):\n","  \n","  tr_loss = 0\n","  n_correct = 0\n","  nb_tr_steps = 0\n","  nb_tr_examples = 0\n","  model.train()\n","  start_time = time.time()\n","  for _,data in enumerate(training_loader, 0):\n","      ids = data['ids'].to(device, dtype = torch.long)\n","      mask = data['mask'].to(device, dtype = torch.long)\n","      targets = data['targets'].to(device, dtype = torch.long)\n","\n","      outputs = model(ids, mask)\n","      loss = loss_function(outputs, targets)\n","      tr_loss += loss.item()\n","      big_val, big_idx = torch.max(outputs.data, dim=1)\n","      n_correct += calcuate_accuracy(big_idx, targets)\n","\n","      nb_tr_steps += 1\n","      nb_tr_examples+=targets.size(0)\n","      \n","      optimizer.zero_grad()\n","      loss.backward()\n","      #When using GPU\n","      optimizer.step()\n","\n","  print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n","  epoch_loss = tr_loss/nb_tr_steps\n","  epoch_accu = (n_correct*100)/nb_tr_examples\n","  print(f\"Training Loss Epoch: {epoch_loss}\")\n","  print(f\"Training Accuracy Epoch: {epoch_accu}\")\n","  end_time = time.time()\n","  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","\n","  return "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGXZaCxBLxkS","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"error","timestamp":1625058939021,"user_tz":-330,"elapsed":720,"user":{"displayName":"Charangan Vasantharajan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gge93DxUsbfBDgym8KngcUFl9bhHWL64xvfuetEUQ=s64","userId":"17392463403142777828"}},"outputId":"6eccee4f-2d8a-4465-e2e2-08973f8941ab"},"source":["for epoch in range(EPOCHS):\n","  train(epoch)\n","  print()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-fa4b900166c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-b3015a84fc18>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-1043f2c3bb18>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     sequence_output, pooled_output = self.auto(input_ids = input_ids, \n\u001b[0;32m---> 17\u001b[0;31m                attention_mask=attention_mask)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# sequence_output has the following shape: (batch_size, sequence_length, 768)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m         )\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         )\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 14.76 GiB total capacity; 13.53 GiB already allocated; 75.75 MiB free; 13.62 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"x6XP-E3f8yRw"},"source":["def valid(model,testing_loader):\n","  model.eval()\n","  n_correct = 0\n","  n_wrong = 0\n","  total = 0\n","  tr_loss = 0\n","  nb_tr_steps = 0\n","  nb_tr_examples = 0\n","  with torch.no_grad():\n","    for _,data in enumerate(testing_loader,0):\n","      ids = data['ids'].to(device,dtype = torch.long)\n","      mask = data['mask'].to(device,dtype = torch.long)\n","      targets = data['targets'].to(device,dtype=torch.long)\n","      outputs = model(ids,mask).squeeze()\n","      loss = loss_function(outputs,targets)\n","      tr_loss += loss.item()\n","      big_val,big_idx = torch.max(outputs.data,dim=1)\n","      n_correct += calcuate_accuracy(big_idx,targets)\n","      nb_tr_steps += 1\n","      nb_tr_examples += targets.size(0)\n","\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_accuracy = (n_correct*100)/nb_tr_examples\n","    print(f\"Validation Loss Epoch:{epoch_loss}\")\n","    print(f\"Validation Accuracy Epoch:{epoch_accuracy}\")\n","\n","    return epoch_accuracy\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SirgRAwcOUo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624987384246,"user_tz":-330,"elapsed":58178,"user":{"displayName":"Testing Test","photoUrl":"","userId":"10745919129946905540"}},"outputId":"90b122f8-baf5-4183-daa0-da3a8316f44c"},"source":["print('This is the validation section to print the accuracy and see how it performs')\n","print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n","\n","acc = valid(model, testing_loader)\n","print(\"Accuracy on test data = %0.2f%%\" % acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is the validation section to print the accuracy and see how it performs\n","Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch\n","Validation Loss Epoch:1.0246658139608122\n","Validation Accuracy Epoch:71.45702902675015\n","Accuracy on test data = 71.46%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V53BQH0GcOtz"},"source":["def get_predictions(model, data_loader):\n","  model = model.eval()\n","  sentence = []\n","  predictions = []\n","  prediction_probs = []\n","  real_values = []\n","  with torch.no_grad():\n","    for d in data_loader:\n","      #texts = d[\"sentences\"]\n","      ids = d[\"ids\"].to(device)\n","      mask = d[\"mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","      outputs = model(\n","        input_ids=ids,\n","        attention_mask=mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","      #sentence.extend(texts)\n","      predictions.extend(preds)\n","      prediction_probs.extend(outputs)\n","      real_values.extend(targets)\n","  predictions = torch.stack(predictions).cpu()\n","  prediction_probs = torch.stack(prediction_probs).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return sentence, predictions, prediction_probs, real_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3x2z768jlSx"},"source":["\n","y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","  model,\n","  testing_loader\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mqXhIN3LxbN"},"source":["class_name = ['Not_offensive','Offensive_Untargetede','Offensive_Targeted_Insult_Group','Offensive_Targeted_Insult_Individual','not-Tamil', 'Offensive_Targeted_Insult_Other']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5iHt3Jfjq2v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624988124292,"user_tz":-330,"elapsed":4,"user":{"displayName":"Testing Test","photoUrl":"","userId":"10745919129946905540"}},"outputId":"5e43ecc4-438c-43a1-9fdb-dd226e851fd3"},"source":["from sklearn.metrics import classification_report,confusion_matrix\n","print(classification_report(y_test, y_pred, target_names=class_name,zero_division=0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                      precision    recall  f1-score   support\n","\n","                       Not_offensive       0.71      1.00      0.83      2511\n","               Offensive_Untargetede       0.00      0.00      0.00       294\n","     Offensive_Targeted_Insult_Group       0.00      0.00      0.00       247\n","Offensive_Targeted_Insult_Individual       0.00      0.00      0.00       253\n","                           not-Tamil       0.00      0.00      0.00       166\n","     Offensive_Targeted_Insult_Other       0.00      0.00      0.00        43\n","\n","                            accuracy                           0.71      3514\n","                           macro avg       0.12      0.17      0.14      3514\n","                        weighted avg       0.51      0.71      0.60      3514\n","\n"],"name":"stdout"}]}]}